{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## algos\n",
    "[regression analysis](https://en.wikipedia.org/wiki/Regression_analysis)\n",
    "\n",
    "[Statistical classification](https://en.wikipedia.org/wiki/Statistical_classification)\n",
    "\n",
    "[supervised learning](https://en.wikipedia.org/wiki/Supervised_learning)\n",
    "\n",
    "[decision tree](https://en.wikipedia.org/wiki/Decision_tree_learning)\n",
    "\n",
    "[random forest](https://en.wikipedia.org/wiki/Random_forest#:~:text=Random%20forests%20or%20random%20decision,regression)\n",
    "\n",
    "[gini inpurity](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity)\n",
    "\n",
    "[entroy - information theory](https://en.wikipedia.org/wiki/Entropy_(information_theory))\n",
    "\n",
    "[Naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n",
    "\n",
    "[Support vector machines](https://en.wikipedia.org/wiki/Support-vector_machine)\n",
    "\n",
    "[logistic regression](https://en.wikipedia.org/wiki/Logistic_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.driver_memory = '6g'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.20:4040\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[*], app id = local-1573155825581)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "init: Int = 2\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val init : Int = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.linalg._\n",
       "import org.apache.spark.mllib.regression._\n",
       "rawData: org.apache.spark.rdd.RDD[String] = hdfs://localhost:9000/ds/covtype/covtype.data MapPartitionsRDD[1] at textFile at <console>:28\n",
       "data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[2] at map at <console>:29\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.linalg._\n",
    "import org.apache.spark.mllib.regression._\n",
    "\n",
    "val rawData = sc.textFile(\"hdfs://localhost:9000/ds/covtype/covtype.data\")\n",
    "val data = rawData.map { line =>\n",
    "    val values = line.split(',').map(_.toDouble)\n",
    "    val featureVector = Vectors.dense(values.init)\n",
    "    val label = values.last - 1\n",
    "    LabeledPoint(label, featureVector)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[3] at randomSplit at <console>:32\n",
       "cvData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[4] at randomSplit at <console>:32\n",
       "testData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[5] at randomSplit at <console>:32\n",
       "res0: testData.type = MapPartitionsRDD[5] at randomSplit at <console>:32\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(trainData, cvData, testData) = data.randomSplit(Array(0.8, 0.1, 0.1))\n",
    "trainData.cache()\n",
    "cvData.cache()\n",
    "testData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.evaluation._\n",
       "import org.apache.spark.mllib.tree._\n",
       "import org.apache.spark.mllib.tree.model._\n",
       "import org.apache.spark.rdd._\n",
       "getMetrics: (model: org.apache.spark.mllib.tree.model.DecisionTreeModel, data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint])org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
       "model: org.apache.spark.mllib.tree.model.DecisionTreeModel = DecisionTreeModel classifier of depth 4 with 23 nodes\n",
       "metrics: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@37c945c\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.evaluation._\n",
    "import org.apache.spark.mllib.tree._\n",
    "import org.apache.spark.mllib.tree.model._\n",
    "import org.apache.spark.rdd._\n",
    "\n",
    "def getMetrics(model: DecisionTreeModel, data: RDD[LabeledPoint]): MulticlassMetrics = {\n",
    "    val predictionAndLabels = data.map(example => \n",
    "        (model.predict(example.features), example.label)\n",
    "    )\n",
    "    new MulticlassMetrics(predictionAndLabels)\n",
    "}\n",
    "val model = DecisionTree.trainClassifier(\n",
    "    trainData, 7, Map[Int, Int](), \"gini\", 4, 100\n",
    ")\n",
    "val metrics = getMetrics(model, cvData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: org.apache.spark.mllib.linalg.Matrix =\n",
       "14177.0  6703.0   9.0     0.0    0.0  2.0   275.0\n",
       "5361.0   22680.0  325.0   18.0   0.0  6.0   34.0\n",
       "0.0      755.0    2730.0  66.0   0.0  14.0  0.0\n",
       "0.0      0.0      167.0   133.0  0.0  0.0   0.0\n",
       "0.0      948.0    11.0    3.0    0.0  0.0   0.0\n",
       "0.0      485.0    1092.0  25.0   0.0  60.0  0.0\n",
       "1137.0   38.0     0.0     0.0    0.0  0.0   855.0\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6857073760580411,0.6698006236416895)\n",
      "(0.7175171628333702,0.7979172530256121)\n",
      "(0.6299030918320259,0.7657784011220197)\n",
      "(0.5428571428571428,0.44333333333333336)\n",
      "(0.0,0.0)\n",
      "(0.7317073170731707,0.036101083032490974)\n",
      "(0.7345360824742269,0.4211822660098522)\n"
     ]
    }
   ],
   "source": [
    "(0 until 7).map(\n",
    "    cat => (metrics.precision(cat), metrics.recall(cat))\n",
    ").foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.rdd._\n",
       "classProbabilities: (data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint])Array[Double]\n",
       "trainPriorProbabilities: Array[Double] = Array(0.3646933869975963, 0.4874675885911841, 0.06154405156585267, 0.004691322987482639, 0.0163056798978315, 0.0300958474696526, 0.0352021224904002)\n",
       "cvPriorProbabilities: Array[Double] = Array(0.36424650226298855, 0.4891497014231875, 0.0613502211361407, 0.005162711456056721, 0.01655509473575522, 0.028601421466554235, 0.03493434751931714)\n",
       "res3: Double = 0.3774433664190958\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.rdd._\n",
    "\n",
    "def classProbabilities(data: RDD[LabeledPoint]): Array[Double] = {\n",
    "    val countByCategory = data.map(_.label).countByValue()\n",
    "    val counts = countByCategory.toArray.sortBy(_._1).map(_._2)\n",
    "    counts.map(_.toDouble / counts.sum)\n",
    "}\n",
    "\n",
    "val trainPriorProbabilities = classProbabilities(trainData)\n",
    "val cvPriorProbabilities = classProbabilities(cvData)\n",
    "trainPriorProbabilities.zip(cvPriorProbabilities).map {\n",
    "    case (trainProb, cvProb) => trainProb * cvProb\n",
    "}.sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluations: Array[((String, Int, Int), Double)] = Array(((gini,1,10),0.6371302207919599), ((gini,1,300),0.6366999948372886), ((gini,20,10),0.8891393760002754), ((gini,20,300),0.9044210019102032), ((entropy,1,10),0.4891497014231875), ((entropy,1,300),0.4891497014231875), ((entropy,20,10),0.8949904489838063), ((entropy,20,300),0.9124748317816517))\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluations =\n",
    "    for (\n",
    "        impurity <- Array(\"gini\", \"entropy\");\n",
    "        depth <- Array(1, 20);\n",
    "        bins <- Array(10, 300)\n",
    "    ) yield {\n",
    "        val model = DecisionTree.trainClassifier(\n",
    "            trainData, 7, Map[Int, Int](), impurity, depth, bins)\n",
    "        val predictionsAnsLabels = cvData.map(example => (model.predict(example.features), example.label)\n",
    "        )\n",
    "        val accuracy = new MulticlassMetrics(predictionsAnsLabels).precision\n",
    "        ((impurity, depth, bins), accuracy)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((entropy,20,300),0.9124748317816517)\n",
      "((gini,20,300),0.9044210019102032)\n",
      "((entropy,20,10),0.8949904489838063)\n",
      "((gini,20,10),0.8891393760002754)\n",
      "((gini,1,10),0.6371302207919599)\n",
      "((gini,1,300),0.6366999948372886)\n",
      "((entropy,1,300),0.4891497014231875)\n",
      "((entropy,1,10),0.4891497014231875)\n"
     ]
    }
   ],
   "source": [
    "evaluations.sortBy(_._2).reverse.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[426] at map at <console>:47\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = rawData.map { line =>\n",
    "    val values = line.split(',').map(_.toDouble)\n",
    "    val wilderness = values.slice(10, 14).indexOf(1.0).toDouble\n",
    "    val soil = values.slice(14, 54).indexOf(1.0).toDouble\n",
    "    val featureVector = Vectors.dense(values.slice(0, 10) :+ wilderness :+ soil)\n",
    "    val label = values.last - 1\n",
    "    LabeledPoint(label, featureVector)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((entropy,30,40),(0.9987142937000392,0.9372042196561634))\n",
      "((entropy,30,300),(0.9985981931311464,0.9406288182553477))\n",
      "((gini,30,40),(0.9974930877161298,0.9344851916226402))\n",
      "((gini,30,300),(0.9952140765489751,0.9328847510712627))\n",
      "((entropy,20,300),(0.9515086623924457,0.9124748317816517))\n",
      "((entropy,20,40),(0.9489501498557343,0.9114594985286273))\n",
      "((gini,20,300),(0.9436202737393413,0.9044210019102032))\n",
      "((gini,20,40),(0.9410058609287185,0.9043005386428953))\n",
      "((gini,10,300),(0.7841389422808172,0.781101034263195))\n",
      "((gini,10,40),(0.7827607855278491,0.78003407389561))\n",
      "((entropy,10,40),(0.7790262172284644,0.778846650260717))\n",
      "((entropy,10,300),(0.774132793250687,0.7705691028928393))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluations: Array[((String, Int, Int), (Double, Double))] = Array(((gini,10,40),(0.7827607855278491,0.78003407389561)), ((gini,10,300),(0.7841389422808172,0.781101034263195)), ((gini,20,40),(0.9410058609287185,0.9043005386428953)), ((gini,20,300),(0.9436202737393413,0.9044210019102032)), ((gini,30,40),(0.9974930877161298,0.9344851916226402)), ((gini,30,300),(0.9952140765489751,0.9328847510712627)), ((entropy,10,40),(0.7790262172284644,0.778846650260717)), ((entropy,10,300),(0.774132793250687,0.7705691028928393)), ((entropy,20,40),(0.9489501498557343,0.9114594985286273)), ((entropy,20,300),(0.9515086623924457,0.9124748317816517)), ((entropy,30,40),(0.9987142937000392,0.9372042196561634)), ((entropy,30,300),(0.9985981931311464,0.9406288182553477)))\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluations =\n",
    "    for (\n",
    "        impurity <- Array(\"gini\", \"entropy\");\n",
    "        depth <- Array(10, 20, 30);\n",
    "        bins <- Array(40, 300)\n",
    "    ) yield {\n",
    "        val model = DecisionTree.trainClassifier(\n",
    "            trainData, 7, Map[Int, Int](10 -> 4, 11 -> 40),\n",
    "            impurity, depth, bins)\n",
    "        val trainAccuracy = getMetrics(model, trainData).precision\n",
    "        val cvAccuracy = getMetrics(model, cvData).precision\n",
    "        ((impurity, depth, bins), (trainAccuracy, cvAccuracy))\n",
    "    }\n",
    "evaluations.sortBy(_._2).reverse.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forest: org.apache.spark.mllib.tree.model.RandomForestModel =\n",
       "TreeEnsembleModel classifier with 20 trees\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val forest = RandomForest.trainClassifier(\n",
    "    trainData, 7, Map(10 -> 4, 11 -> 40), 20,\n",
    "    \"auto\", \"entropy\", 30, 300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "java.lang.ArrayIndexOutOfBoundsException",
     "evalue": " 36",
     "output_type": "error",
     "traceback": [
      "java.lang.ArrayIndexOutOfBoundsException: 36",
      "  at org.apache.spark.mllib.linalg.DenseVector.apply(Vectors.scala:630)",
      "  at org.apache.spark.mllib.tree.model.Node.predict(Node.scala:70)",
      "  at org.apache.spark.mllib.tree.model.DecisionTreeModel.predict(DecisionTreeModel.scala:57)",
      "  at org.apache.spark.mllib.tree.model.TreeEnsembleModel$$anonfun$predictByVoting$1.apply(treeEnsembleModels.scala:304)",
      "  at org.apache.spark.mllib.tree.model.TreeEnsembleModel$$anonfun$predictByVoting$1.apply(treeEnsembleModels.scala:303)",
      "  at scala.collection.Iterator$class.foreach(Iterator.scala:891)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)",
      "  at scala.collection.IterableViewLike$Transformed$class.foreach(IterableViewLike.scala:44)",
      "  at scala.collection.SeqViewLike$AbstractTransformed.foreach(SeqViewLike.scala:37)",
      "  at org.apache.spark.mllib.tree.model.TreeEnsembleModel.predictByVoting(treeEnsembleModels.scala:303)",
      "  at org.apache.spark.mllib.tree.model.TreeEnsembleModel.predict(treeEnsembleModels.scala:327)",
      "  ... 50 elided",
      ""
     ]
    }
   ],
   "source": [
    "val input = \"2709,125,28,67,23,3224,253,207,61,6094,0,29\"\n",
    "val vector = Vectors.dense(input.split(',').map(_.toDouble))\n",
    "forest.predict(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
