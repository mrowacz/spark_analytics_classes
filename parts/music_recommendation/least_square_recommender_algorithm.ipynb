{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### links\n",
    "[Collaborative filtering](https://en.wikipedia.org/wiki/Collaborative_filtering)\n",
    "\n",
    "latent-factor [Factor analysis](https://en.wikipedia.org/wiki/Factor_analysis)\n",
    "\n",
    "[Matrix factorization](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization)\n",
    "\n",
    "[QR decomposition](https://en.wikipedia.org/wiki/QR_decomposition)\n",
    "\n",
    "[ROC curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)\n",
    "\n",
    "[Evaluation measures](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision)\n",
    "\n",
    "[k-fold cross-validation](https://machinelearningmastery.com/k-fold-cross-validation/)\n",
    "\n",
    "see\n",
    "\n",
    "```OryxProject```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.driver_memory = '6g'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://czu8001-precision-5520.home:4040\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[*], app id = local-1572904279943)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "rawUserArtistData: org.apache.spark.rdd.RDD[String] = ../../data/music/user_artist_data.txt MapPartitionsRDD[1] at textFile at <console>:25\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawUserArtistData = sc.textFile(\"../../data/music/user_artist_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.util.StatCounter = (count: 24296858, mean: 1718704.093757, stdev: 2539389.040171, max: 10794401.000000, min: 1.000000)\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawUserArtistData.map(_.split(\" \")(0).toDouble).stats\n",
    "rawUserArtistData.map(_.split(\" \")(1).toDouble).stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rawArtistData: org.apache.spark.rdd.RDD[String] = ../../data/music/artist_data.txt MapPartitionsRDD[7] at textFile at <console>:25\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawArtistData = sc.textFile(\"../../data/music/artist_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artistByID: org.apache.spark.rdd.RDD[Option[(Int, String)]] = MapPartitionsRDD[8] at map at <console>:26\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val artistByID = rawArtistData.map { line =>\n",
    "    val (id, name) = line.span(_ != '\\t')\n",
    "    if (name.isEmpty) {\n",
    "        None\n",
    "    } else {\n",
    "        try {\n",
    "            Some((id.toInt, name.trim))\n",
    "        } catch {\n",
    "            case e: NumberFormatException => None\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rawArtistAlias: org.apache.spark.rdd.RDD[String] = ../../data/music/artist_alias.txt MapPartitionsRDD[10] at textFile at <console>:25\n",
       "artistAlias: scala.collection.Map[Int,Int] = Map(6803336 -> 1000010, 6663187 -> 1992, 2124273 -> 2814, 10412283 -> 1010353, 9969191 -> 1320354, 2024757 -> 1001941, 10208201 -> 4605, 2139121 -> 1011083, 1186393 -> 78, 2094504 -> 1012167, 9931106 -> 1000289, 2167517 -> 2060894, 1351735 -> 1266817, 6943682 -> 1003342, 2027368 -> 1000024, 2056419 -> 1020783, 1214789 -> 1001066, 1022944 -> 1004983, 6640739 -> 1010367, 6902331 -> 411, 10303141 -> 82, 10029249 -> 2070, 7001129 -> 739, 6627784 -> 1046699, 1113560 -> 1275800, 2155414 -> 1000790, 1291139 -> 4163, 10061700 -> 831, 1043158 -> 1301875, 10294241 -> 1234737, 9991298 -> 1001419, 9965450 -> 1016520, 68004..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawArtistAlias = sc.textFile(\"../../data/music/artist_alias.txt\")\n",
    "val artistAlias = rawArtistAlias.flatMap { line =>\n",
    "    val tokens = line.split('\\t')\n",
    "    if (tokens(0).isEmpty) {\n",
    "        None\n",
    "    } else {\n",
    "        Some((tokens(0).toInt, tokens(1).toInt))\n",
    "    }\n",
    "}.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.recommendation._\n",
       "bArtistAlias: org.apache.spark.broadcast.Broadcast[scala.collection.Map[Int,Int]] = Broadcast(6)\n",
       "trainData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[12] at map at <console>:33\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.recommendation._\n",
    "\n",
    "val bArtistAlias = sc.broadcast(artistAlias)\n",
    "\n",
    "val trainData = rawUserArtistData.map { line =>\n",
    "    val Array(userID, artistID, count) = line.split(' ').map(_.toInt)\n",
    "    val finalArtistID = bArtistAlias.value.getOrElse(artistID, artistID)\n",
    "    Rating(userID, finalArtistID, count)\n",
    "}.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.mllib.recommendation.MatrixFactorizationModel = org.apache.spark.mllib.recommendation.MatrixFactorizationModel@2fca6529\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = ALS.trainImplicit(trainData, 10, 5, 0.01, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rawArtistForUser: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[141] at filter at <console>:30\n",
       "existingProducts: scala.collection.immutable.Set[Int] = Set(1255340, 942, 1180, 813, 378)\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// checking model\n",
    "val rawArtistForUser = rawUserArtistData.map(_.split(' ')).filter{ case Array(user,_,_) => user.toInt == 2093760 }\n",
    "val existingProducts = rawArtistForUser.map { case Array(_,artist,_) => artist.toInt }.collect().toSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some((1180,David Gray))\n",
      "Some((378,Blackalicious))\n",
      "Some((813,Jurassic 5))\n",
      "Some((1255340,The Saw Doctors))\n",
      "Some((942,Xzibit))\n"
     ]
    }
   ],
   "source": [
    "artistByID.filter(x => x != None).filter(x => existingProducts.contains(x.get._1)).collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating(2093760,1001819,0.02628420179655294)\n",
      "Rating(2093760,2814,0.026010115646986007)\n",
      "Rating(2093760,1811,0.024987081528557605)\n",
      "Rating(2093760,1300642,0.024822425820989272)\n",
      "Rating(2093760,4605,0.024688995357883187)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "recommendations: Array[org.apache.spark.mllib.recommendation.Rating] = Array(Rating(2093760,1001819,0.02628420179655294), Rating(2093760,2814,0.026010115646986007), Rating(2093760,1811,0.024987081528557605), Rating(2093760,1300642,0.024822425820989272), Rating(2093760,4605,0.024688995357883187))\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val recommendations = model.recommendProducts(2093760, 5)\n",
    "recommendations.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "recommendedProductsIDs: scala.collection.immutable.Set[Int] = Set(2814, 1811, 1001819, 1300642, 4605)\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val recommendedProductsIDs = recommendations.map(_.product).toSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Array[Option[(Int, String)]] = Array(Some((2814,50 Cent)), Some((4605,Snoop Dogg)), Some((1811,Dr. Dre)), Some((1001819,2Pac)), Some((1300642,The Game)))\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artistByID.filter{x => x != None}.filter {x => recommendedProductsIDs.contains(x.get._1) }.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.rdd._\n",
       "import scala.collection.Map\n",
       "import scala.collection.mutable.ArrayBuffer\n",
       "import scala.util.Random\n",
       "import org.apache.spark.broadcast.Broadcast\n",
       "import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}\n",
       "import org.apache.spark.sql.functions._\n",
       "areaUnderCurve: (positiveData: org.apache.spark.sql.DataFrame, bAllArtistIDs: org.apache.spark.broadcast.Broadcast[Array[Int]], predictFunction: org.apache.spark.sql.DataFrame => org.apache.spark.sql.DataFrame)Double\n",
       "buildRatings: (rawUserArtistData: org.apache.spark.rdd.RDD[String], bArtistAlias: org.apache.spark.broadcast.Broadcast[scala.collection.Map[Int,Int]])org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.rdd._\n",
    "\n",
    "import scala.collection.Map\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "import scala.util.Random\n",
    "import org.apache.spark.broadcast.Broadcast\n",
    "//import org.apache.spark.ml.recommendation.{ALS, ALSModel}\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "  def areaUnderCurve(\n",
    "      positiveData: DataFrame,\n",
    "      bAllArtistIDs: Broadcast[Array[Int]],\n",
    "      predictFunction: (DataFrame => DataFrame)): Double = {\n",
    "\n",
    "    // What this actually computes is AUC, per user. The result is actually something\n",
    "    // that might be called \"mean AUC\".\n",
    "\n",
    "    // Take held-out data as the \"positive\".\n",
    "    // Make predictions for each of them, including a numeric score\n",
    "    val positivePredictions = predictFunction(positiveData.select(\"user\", \"artist\")).\n",
    "      withColumnRenamed(\"prediction\", \"positivePrediction\")\n",
    "\n",
    "    // BinaryClassificationMetrics.areaUnderROC is not used here since there are really lots of\n",
    "    // small AUC problems, and it would be inefficient, when a direct computation is available.\n",
    "\n",
    "    // Create a set of \"negative\" products for each user. These are randomly chosen\n",
    "    // from among all of the other artists, excluding those that are \"positive\" for the user.\n",
    "    val negativeData = positiveData.select(\"user\", \"artist\").as[(Int,Int)].\n",
    "      groupByKey { case (user, _) => user }.\n",
    "      flatMapGroups { case (userID, userIDAndPosArtistIDs) =>\n",
    "        val random = new Random()\n",
    "        val posItemIDSet = userIDAndPosArtistIDs.map { case (_, artist) => artist }.toSet\n",
    "        val negative = new ArrayBuffer[Int]()\n",
    "        val allArtistIDs = bAllArtistIDs.value\n",
    "        var i = 0\n",
    "        // Make at most one pass over all artists to avoid an infinite loop.\n",
    "        // Also stop when number of negative equals positive set size\n",
    "        while (i < allArtistIDs.length && negative.size < posItemIDSet.size) {\n",
    "          val artistID = allArtistIDs(random.nextInt(allArtistIDs.length))\n",
    "          // Only add new distinct IDs\n",
    "          if (!posItemIDSet.contains(artistID)) {\n",
    "            negative += artistID\n",
    "          }\n",
    "          i += 1\n",
    "        }\n",
    "        // Return the set with user ID added back\n",
    "        negative.map(artistID => (userID, artistID))\n",
    "      }.toDF(\"user\", \"artist\")\n",
    "\n",
    "    // Make predictions on the rest:\n",
    "    val negativePredictions = predictFunction(negativeData).\n",
    "      withColumnRenamed(\"prediction\", \"negativePrediction\")\n",
    "\n",
    "    // Join positive predictions to negative predictions by user, only.\n",
    "    // This will result in a row for every possible pairing of positive and negative\n",
    "    // predictions within each user.\n",
    "    val joinedPredictions = positivePredictions.join(negativePredictions, \"user\").\n",
    "      select(\"user\", \"positivePrediction\", \"negativePrediction\").cache()\n",
    "\n",
    "    // Count the number of pairs per user\n",
    "    val allCounts = joinedPredictions.\n",
    "      groupBy(\"user\").agg(count(lit(\"1\")).as(\"total\")).\n",
    "      select(\"user\", \"total\")\n",
    "    // Count the number of correctly ordered pairs per user\n",
    "    val correctCounts = joinedPredictions.\n",
    "      filter($\"positivePrediction\" > $\"negativePrediction\").\n",
    "      groupBy(\"user\").agg(count(\"user\").as(\"correct\")).\n",
    "      select(\"user\", \"correct\")\n",
    "\n",
    "    // Combine these, compute their ratio, and average over all users\n",
    "    val meanAUC = allCounts.join(correctCounts, Seq(\"user\"), \"left_outer\").\n",
    "      select($\"user\", (coalesce($\"correct\", lit(0)) / $\"total\").as(\"auc\")).\n",
    "      agg(mean(\"auc\")).\n",
    "      as[Double].first()\n",
    "\n",
    "    joinedPredictions.unpersist()\n",
    "\n",
    "    meanAUC\n",
    "  }\n",
    "\n",
    "\n",
    "def buildRatings(\n",
    "      rawUserArtistData: RDD[String],\n",
    "      bArtistAlias: Broadcast[Map[Int,Int]]) = {\n",
    "    rawUserArtistData.map { line =>\n",
    "      val Array(userID, artistID, count) = line.split(' ').map(_.toInt)\n",
    "      val finalArtistID = bArtistAlias.value.getOrElse(artistID, artistID)\n",
    "      Rating(userID, finalArtistID, count)\n",
    "    }\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "allData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[302] at map at <console>:116\n",
       "trainData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[303] at randomSplit at <console>:51\n",
       "cvData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[304] at randomSplit at <console>:51\n",
       "allItemIDs: Array[Int] = Array(1115179, 2157922, 6697353, 9979788, 10486944, 2292732, 2094547, 10132642, 10269896, 10692487, 10124491, 10784358, 10537033, 6848803, 10526529, 6740630, 1102283, 1276561, 10449985, 6981052, 6764186, 9959690, 10090912, 9961692, 10186813, 6657404, 6944769, 10468315, 1082679, 10224097, 7037004, 2116751, 10661235, 10384998, 9993152, 2043119, 6811740, 1216787, 2108067, 1..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val allData = buildRatings(rawUserArtistData, bArtistAlias)\n",
    "val Array(trainData, cvData) = allData.randomSplit(Array(0.9, 0.1))\n",
    "trainData.cache()\n",
    "cvData.cache()\n",
    "\n",
    "val allItemIDs = allData.map(_.product).distinct().collect()\n",
    "val bAllItemIDS = sc.broadcast(allItemIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.mllib.recommendation.MatrixFactorizationModel = org.apache.spark.mllib.recommendation.MatrixFactorizationModel@7f87277a\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = ALS.trainImplicit(trainData, 10, 5, 0.01, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "45: error: value transform is not a member of org.apache.spark.mllib.recommendation.MatrixFactorizationModel",
     "output_type": "error",
     "traceback": [
      "<console>:45: error: value transform is not a member of org.apache.spark.mllib.recommendation.MatrixFactorizationModel",
      "       val auc = areaUnderCurve(cvData.toDF, bAllItemIDS, model.transform)",
      "                                                                ^",
      ""
     ]
    }
   ],
   "source": [
    "val auc = areaUnderCurve(cvData.toDF, bAllItemIDS, model.transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: org.apache.spark.mllib.recommendation.MatrixFactorizationModel = org.apache.spark.mllib.recommendation.MatrixFactorizationModel@7f87277a\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
