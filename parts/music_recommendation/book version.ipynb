{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "46: error: not enough arguments for method main: (args: Array[String])Unit.",
     "output_type": "error",
     "traceback": [
      "<console>:46: error: not enough arguments for method main: (args: Array[String])Unit.",
      "Unspecified value parameter args.",
      "       RunRecommender.main()",
      "                          ^",
      ""
     ]
    }
   ],
   "source": [
    "RunRecommender.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.collection.Map\n",
       "import scala.collection.mutable.ArrayBuffer\n",
       "import scala.util.Random\n",
       "import org.apache.spark.broadcast.Broadcast\n",
       "import org.apache.spark.ml.recommendation.{ALS, ALSModel}\n",
       "import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}\n",
       "import org.apache.spark.sql.functions._\n",
       "defined object RunRecommender\n",
       "defined class RunRecommender\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*\n",
    " * Copyright 2015 and onwards Sanford Ryza, Uri Laserson, Sean Owen and Joshua Wills\n",
    " *\n",
    " * See LICENSE file for further information.\n",
    " */\n",
    "\n",
    "import scala.collection.Map\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "import scala.util.Random\n",
    "import org.apache.spark.broadcast.Broadcast\n",
    "import org.apache.spark.ml.recommendation.{ALS, ALSModel}\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "object RunRecommender {\n",
    "\n",
    "  def main(): Unit = {\n",
    "//     val spark = SparkSession.builder().getOrCreate()\n",
    "    // Optional, but may help avoid errors due to long lineage\n",
    "//     spark.sparkContext.setCheckpointDir(\"hdfs:///tmp/\")\n",
    "\n",
    "    val base = \"../../data/music/\"\n",
    "    val rawUserArtistData = sc.read.textFile(base + \"user_artist_data.txt\")\n",
    "    val rawArtistData = sc.read.textFile(base + \"artist_data.txt\")\n",
    "    val rawArtistAlias = sc.read.textFile(base + \"artist_alias.txt\")\n",
    "\n",
    "    val runRecommender = new RunRecommender()\n",
    "    runRecommender.preparation(rawUserArtistData, rawArtistData, rawArtistAlias)\n",
    "    runRecommender.model(rawUserArtistData, rawArtistData, rawArtistAlias)\n",
    "    runRecommender.evaluate(rawUserArtistData, rawArtistAlias)\n",
    "    runRecommender.recommend(rawUserArtistData, rawArtistData, rawArtistAlias)\n",
    "  }\n",
    "\n",
    "}\n",
    "\n",
    "class RunRecommender(private val spark: SparkSession) {\n",
    "\n",
    "  import spark.implicits._\n",
    "\n",
    "  def preparation(\n",
    "      rawUserArtistData: Dataset[String],\n",
    "      rawArtistData: Dataset[String],\n",
    "      rawArtistAlias: Dataset[String]): Unit = {\n",
    "\n",
    "    rawUserArtistData.take(5).foreach(println)\n",
    "\n",
    "    val userArtistDF = rawUserArtistData.map { line =>\n",
    "      val Array(user, artist, _*) = line.split(' ')\n",
    "      (user.toInt, artist.toInt)\n",
    "    }.toDF(\"user\", \"artist\")\n",
    "\n",
    "    userArtistDF.agg(min(\"user\"), max(\"user\"), min(\"artist\"), max(\"artist\")).show()\n",
    "\n",
    "    val artistByID = buildArtistByID(rawArtistData)\n",
    "    val artistAlias = buildArtistAlias(rawArtistAlias)\n",
    "\n",
    "    val (badID, goodID) = artistAlias.head\n",
    "    artistByID.filter($\"id\" isin (badID, goodID)).show()\n",
    "  }\n",
    "\n",
    "  def model(\n",
    "      rawUserArtistData: Dataset[String],\n",
    "      rawArtistData: Dataset[String],\n",
    "      rawArtistAlias: Dataset[String]): Unit = {\n",
    "\n",
    "    val bArtistAlias = spark.sparkContext.broadcast(buildArtistAlias(rawArtistAlias))\n",
    "\n",
    "    val trainData = buildCounts(rawUserArtistData, bArtistAlias).cache()\n",
    "\n",
    "    val model = new ALS().\n",
    "      setSeed(Random.nextLong()).\n",
    "      setImplicitPrefs(true).\n",
    "      setRank(10).\n",
    "      setRegParam(0.01).\n",
    "      setAlpha(1.0).\n",
    "      setMaxIter(5).\n",
    "      setUserCol(\"user\").\n",
    "      setItemCol(\"artist\").\n",
    "      setRatingCol(\"count\").\n",
    "      setPredictionCol(\"prediction\").\n",
    "      fit(trainData)\n",
    "\n",
    "    trainData.unpersist()\n",
    "\n",
    "    model.userFactors.select(\"features\").show(truncate = false)\n",
    "\n",
    "    val userID = 2093760\n",
    "\n",
    "    val existingArtistIDs = trainData.\n",
    "      filter($\"user\" === userID).\n",
    "      select(\"artist\").as[Int].collect()\n",
    "\n",
    "    val artistByID = buildArtistByID(rawArtistData)\n",
    "\n",
    "    artistByID.filter($\"id\" isin (existingArtistIDs:_*)).show()\n",
    "\n",
    "    val topRecommendations = makeRecommendations(model, userID, 5)\n",
    "    topRecommendations.show()\n",
    "\n",
    "    val recommendedArtistIDs = topRecommendations.select(\"artist\").as[Int].collect()\n",
    "\n",
    "    artistByID.filter($\"id\" isin (recommendedArtistIDs:_*)).show()\n",
    "\n",
    "    model.userFactors.unpersist()\n",
    "    model.itemFactors.unpersist()\n",
    "  }\n",
    "\n",
    "  def evaluate(\n",
    "      rawUserArtistData: Dataset[String],\n",
    "      rawArtistAlias: Dataset[String]): Unit = {\n",
    "\n",
    "    val bArtistAlias = spark.sparkContext.broadcast(buildArtistAlias(rawArtistAlias))\n",
    "\n",
    "    val allData = buildCounts(rawUserArtistData, bArtistAlias)\n",
    "    val Array(trainData, cvData) = allData.randomSplit(Array(0.9, 0.1))\n",
    "    trainData.cache()\n",
    "    cvData.cache()\n",
    "\n",
    "    val allArtistIDs = allData.select(\"artist\").as[Int].distinct().collect()\n",
    "    val bAllArtistIDs = spark.sparkContext.broadcast(allArtistIDs)\n",
    "\n",
    "    val mostListenedAUC = areaUnderCurve(cvData, bAllArtistIDs, predictMostListened(trainData))\n",
    "    println(mostListenedAUC)\n",
    "\n",
    "    val evaluations =\n",
    "      for (rank     <- Seq(5,  30);\n",
    "           regParam <- Seq(1.0, 0.0001);\n",
    "           alpha    <- Seq(1.0, 40.0))\n",
    "      yield {\n",
    "        val model = new ALS().\n",
    "          setSeed(Random.nextLong()).\n",
    "          setImplicitPrefs(true).\n",
    "          setRank(rank).setRegParam(regParam).\n",
    "          setAlpha(alpha).setMaxIter(20).\n",
    "          setUserCol(\"user\").setItemCol(\"artist\").\n",
    "          setRatingCol(\"count\").setPredictionCol(\"prediction\").\n",
    "          fit(trainData)\n",
    "\n",
    "        val auc = areaUnderCurve(cvData, bAllArtistIDs, model.transform)\n",
    "\n",
    "        model.userFactors.unpersist()\n",
    "        model.itemFactors.unpersist()\n",
    "\n",
    "        (auc, (rank, regParam, alpha))\n",
    "      }\n",
    "\n",
    "    evaluations.sorted.reverse.foreach(println)\n",
    "\n",
    "    trainData.unpersist()\n",
    "    cvData.unpersist()\n",
    "  }\n",
    "\n",
    "  def recommend(\n",
    "      rawUserArtistData: Dataset[String],\n",
    "      rawArtistData: Dataset[String],\n",
    "      rawArtistAlias: Dataset[String]): Unit = {\n",
    "\n",
    "    val bArtistAlias = spark.sparkContext.broadcast(buildArtistAlias(rawArtistAlias))\n",
    "    val allData = buildCounts(rawUserArtistData, bArtistAlias).cache()\n",
    "    val model = new ALS().\n",
    "      setSeed(Random.nextLong()).\n",
    "      setImplicitPrefs(true).\n",
    "      setRank(10).setRegParam(1.0).setAlpha(40.0).setMaxIter(20).\n",
    "      setUserCol(\"user\").setItemCol(\"artist\").\n",
    "      setRatingCol(\"count\").setPredictionCol(\"prediction\").\n",
    "      fit(allData)\n",
    "    allData.unpersist()\n",
    "\n",
    "    val userID = 2093760\n",
    "    val topRecommendations = makeRecommendations(model, userID, 5)\n",
    "\n",
    "    val recommendedArtistIDs = topRecommendations.select(\"artist\").as[Int].collect()\n",
    "    val artistByID = buildArtistByID(rawArtistData)\n",
    "    artistByID.join(spark.createDataset(recommendedArtistIDs).toDF(\"id\"), \"id\").\n",
    "      select(\"name\").show()\n",
    "\n",
    "    model.userFactors.unpersist()\n",
    "    model.itemFactors.unpersist()\n",
    "  }\n",
    "\n",
    "  def buildArtistByID(rawArtistData: Dataset[String]): DataFrame = {\n",
    "    rawArtistData.flatMap { line =>\n",
    "      val (id, name) = line.span(_ != '\\t')\n",
    "      if (name.isEmpty) {\n",
    "        None\n",
    "      } else {\n",
    "        try {\n",
    "          Some((id.toInt, name.trim))\n",
    "        } catch {\n",
    "          case _: NumberFormatException => None\n",
    "        }\n",
    "      }\n",
    "    }.toDF(\"id\", \"name\")\n",
    "  }\n",
    "\n",
    "  def buildArtistAlias(rawArtistAlias: Dataset[String]): Map[Int,Int] = {\n",
    "    rawArtistAlias.flatMap { line =>\n",
    "      val Array(artist, alias) = line.split('\\t')\n",
    "      if (artist.isEmpty) {\n",
    "        None\n",
    "      } else {\n",
    "        Some((artist.toInt, alias.toInt))\n",
    "      }\n",
    "    }.collect().toMap\n",
    "  }\n",
    "\n",
    "  def buildCounts(\n",
    "      rawUserArtistData: Dataset[String],\n",
    "      bArtistAlias: Broadcast[Map[Int,Int]]): DataFrame = {\n",
    "    rawUserArtistData.map { line =>\n",
    "      val Array(userID, artistID, count) = line.split(' ').map(_.toInt)\n",
    "      val finalArtistID = bArtistAlias.value.getOrElse(artistID, artistID)\n",
    "      (userID, finalArtistID, count)\n",
    "    }.toDF(\"user\", \"artist\", \"count\")\n",
    "  }\n",
    "\n",
    "  def makeRecommendations(model: ALSModel, userID: Int, howMany: Int): DataFrame = {\n",
    "    val toRecommend = model.itemFactors.\n",
    "      select($\"id\".as(\"artist\")).\n",
    "      withColumn(\"user\", lit(userID))\n",
    "    model.transform(toRecommend).\n",
    "      select(\"artist\", \"prediction\").\n",
    "      orderBy($\"prediction\".desc).\n",
    "      limit(howMany)\n",
    "  }\n",
    "\n",
    "  def areaUnderCurve(\n",
    "      positiveData: DataFrame,\n",
    "      bAllArtistIDs: Broadcast[Array[Int]],\n",
    "      predictFunction: (DataFrame => DataFrame)): Double = {\n",
    "\n",
    "    // What this actually computes is AUC, per user. The result is actually something\n",
    "    // that might be called \"mean AUC\".\n",
    "\n",
    "    // Take held-out data as the \"positive\".\n",
    "    // Make predictions for each of them, including a numeric score\n",
    "    val positivePredictions = predictFunction(positiveData.select(\"user\", \"artist\")).\n",
    "      withColumnRenamed(\"prediction\", \"positivePrediction\")\n",
    "\n",
    "    // BinaryClassificationMetrics.areaUnderROC is not used here since there are really lots of\n",
    "    // small AUC problems, and it would be inefficient, when a direct computation is available.\n",
    "\n",
    "    // Create a set of \"negative\" products for each user. These are randomly chosen\n",
    "    // from among all of the other artists, excluding those that are \"positive\" for the user.\n",
    "    val negativeData = positiveData.select(\"user\", \"artist\").as[(Int,Int)].\n",
    "      groupByKey { case (user, _) => user }.\n",
    "      flatMapGroups { case (userID, userIDAndPosArtistIDs) =>\n",
    "        val random = new Random()\n",
    "        val posItemIDSet = userIDAndPosArtistIDs.map { case (_, artist) => artist }.toSet\n",
    "        val negative = new ArrayBuffer[Int]()\n",
    "        val allArtistIDs = bAllArtistIDs.value\n",
    "        var i = 0\n",
    "        // Make at most one pass over all artists to avoid an infinite loop.\n",
    "        // Also stop when number of negative equals positive set size\n",
    "        while (i < allArtistIDs.length && negative.size < posItemIDSet.size) {\n",
    "          val artistID = allArtistIDs(random.nextInt(allArtistIDs.length))\n",
    "          // Only add new distinct IDs\n",
    "          if (!posItemIDSet.contains(artistID)) {\n",
    "            negative += artistID\n",
    "          }\n",
    "          i += 1\n",
    "        }\n",
    "        // Return the set with user ID added back\n",
    "        negative.map(artistID => (userID, artistID))\n",
    "      }.toDF(\"user\", \"artist\")\n",
    "\n",
    "    // Make predictions on the rest:\n",
    "    val negativePredictions = predictFunction(negativeData).\n",
    "      withColumnRenamed(\"prediction\", \"negativePrediction\")\n",
    "\n",
    "    // Join positive predictions to negative predictions by user, only.\n",
    "    // This will result in a row for every possible pairing of positive and negative\n",
    "    // predictions within each user.\n",
    "    val joinedPredictions = positivePredictions.join(negativePredictions, \"user\").\n",
    "      select(\"user\", \"positivePrediction\", \"negativePrediction\").cache()\n",
    "\n",
    "    // Count the number of pairs per user\n",
    "    val allCounts = joinedPredictions.\n",
    "      groupBy(\"user\").agg(count(lit(\"1\")).as(\"total\")).\n",
    "      select(\"user\", \"total\")\n",
    "    // Count the number of correctly ordered pairs per user\n",
    "    val correctCounts = joinedPredictions.\n",
    "      filter($\"positivePrediction\" > $\"negativePrediction\").\n",
    "      groupBy(\"user\").agg(count(\"user\").as(\"correct\")).\n",
    "      select(\"user\", \"correct\")\n",
    "\n",
    "    // Combine these, compute their ratio, and average over all users\n",
    "    val meanAUC = allCounts.join(correctCounts, Seq(\"user\"), \"left_outer\").\n",
    "      select($\"user\", (coalesce($\"correct\", lit(0)) / $\"total\").as(\"auc\")).\n",
    "      agg(mean(\"auc\")).\n",
    "      as[Double].first()\n",
    "\n",
    "    joinedPredictions.unpersist()\n",
    "\n",
    "    meanAUC\n",
    "  }\n",
    "\n",
    "  def predictMostListened(train: DataFrame)(allData: DataFrame): DataFrame = {\n",
    "    val listenCounts = train.groupBy(\"artist\").\n",
    "      agg(sum(\"count\").as(\"prediction\")).\n",
    "      select(\"artist\", \"prediction\")\n",
    "    allData.\n",
    "      join(listenCounts, Seq(\"artist\"), \"left_outer\").\n",
    "      select(\"user\", \"artist\", \"prediction\")\n",
    "  }\n",
    "\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
